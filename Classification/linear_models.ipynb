{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM0VbPQejEgKSqhNdmaNujD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"PCwRQBE08zmj","executionInfo":{"status":"ok","timestamp":1745584644144,"user_tz":-660,"elapsed":10932,"user":{"displayName":"Jiyuan Chen","userId":"11548750978247571507"}}},"outputs":[],"source":["# linear_models.py\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class LinearTemplateClassifier(nn.Module):\n","    \"\"\"\n","    A linear classifier for predicting the SQL template ID given a sentence.\n","    This uses a bag-of-words embedding averaged over tokens, passed to a linear layer.\n","    \"\"\"\n","    def __init__(self, vocab_size, embedding_dim, num_templates):\n","        super(LinearTemplateClassifier, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.linear = nn.Linear(embedding_dim, num_templates)\n","\n","    def forward(self, input_ids):\n","        \"\"\"\n","        input_ids: (batch_size, seq_len)\n","        Returns: (batch_size, num_templates)\n","        \"\"\"\n","        # Embed and average over time (simple bag-of-words)\n","        embedded = self.embedding(input_ids)  # (batch_size, seq_len, emb_dim)\n","        mean_emb = embedded.mean(dim=1)  # (batch_size, emb_dim)\n","        logits = self.linear(mean_emb)   # (batch_size, num_templates)\n","        return logits\n","\n","\n","class LinearSequenceTagger(nn.Module):\n","    \"\"\"\n","    A linear tagger for variable detection â€” predicts a label per word.\n","    \"\"\"\n","    def __init__(self, vocab_size, embedding_dim, num_tags):\n","        super(LinearSequenceTagger, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.linear = nn.Linear(embedding_dim, num_tags)\n","\n","    def forward(self, input_ids):\n","        \"\"\"\n","        input_ids: (batch_size, seq_len)\n","        Returns: (batch_size, seq_len, num_tags)\n","        \"\"\"\n","        embedded = self.embedding(input_ids)  # (batch_size, seq_len, emb_dim)\n","        logits = self.linear(embedded)        # (batch_size, seq_len, num_tags)\n","        return logits\n","\n"]}]}